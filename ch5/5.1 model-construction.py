# å›é¡¾MLPä»£ç 
# ç”Ÿæˆä¸€ä¸ªç½‘ç»œï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªå…·æœ‰256ä¸ªå•å…ƒå’ŒReLUæ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥éšè—å±‚
# ç„¶åæ˜¯ä¸€ä¸ªå…·æœ‰10ä¸ªéšè—å•å…ƒä¸”ä¸å¸¦æ¿€æ´»å‡½æ•°çš„å…¨è¿æ¥è¾“å‡ºå±‚ã€‚
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)

# è‡ªå®šä¹‰å—åŸºæœ¬åŠŸèƒ½
# 1. å°†è¾“å…¥æ•°æ®ä½œä¸ºå…¶å‰å‘ä¼ æ’­å‡½æ•°çš„å‚æ•°ã€‚
# 2. é€šè¿‡å‰å‘ä¼ æ’­å‡½æ•°æ¥ç”Ÿæˆè¾“å‡ºã€‚è¯·æ³¨æ„ï¼Œè¾“å‡ºçš„å½¢çŠ¶å¯èƒ½ä¸è¾“å…¥çš„å½¢çŠ¶ä¸åŒã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬ä¸Šé¢æ¨¡å‹ä¸­çš„ç¬¬ä¸€ä¸ªå…¨è¿æ¥çš„å±‚æ¥æ”¶ä¸€ä¸ª20ç»´çš„è¾“å…¥ï¼Œä½†æ˜¯è¿”å›ä¸€ä¸ªç»´åº¦ä¸º256çš„è¾“å‡ºã€‚
# 3. è®¡ç®—å…¶è¾“å‡ºå…³äºè¾“å…¥çš„æ¢¯åº¦ï¼Œå¯é€šè¿‡å…¶åå‘ä¼ æ’­å‡½æ•°è¿›è¡Œè®¿é—®ã€‚é€šå¸¸è¿™æ˜¯è‡ªåŠ¨å‘ç”Ÿçš„ã€‚
# 4. å­˜å‚¨å’Œè®¿é—®å‰å‘ä¼ æ’­è®¡ç®—æ‰€éœ€çš„å‚æ•°ã€‚
# 5. æ ¹æ®éœ€è¦åˆå§‹åŒ–æ¨¡å‹å‚æ•°ã€‚

# ä¸‹é¢è‡ªå®šä¹‰å—åŒ…å«ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼Œå…¶å…·æœ‰256ä¸ªéšè—å•å…ƒçš„éšè—å±‚å’Œä¸€ä¸ª10ç»´è¾“å‡ºå±‚ã€‚ 
# æ³¨æ„ï¼Œä¸‹é¢çš„MLPç±»ç»§æ‰¿äº†è¡¨ç¤ºå—çš„ç±»ã€‚ 
# æˆ‘ä»¬çš„å®ç°åªéœ€è¦æä¾›æˆ‘ä»¬è‡ªå·±çš„æ„é€ å‡½æ•°ï¼ˆPythonä¸­çš„__init__å‡½æ•°ï¼‰å’Œå‰å‘ä¼ æ’­å‡½æ•°ã€‚
class MLP(nn.Module):
    # ç”¨æ¨¡å‹å‚æ•°å£°æ˜å±‚ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å£°æ˜ä¸¤ä¸ªå…¨è¿æ¥çš„å±‚
    def __init__(self):
        # è°ƒç”¨MLPçš„çˆ¶ç±»Moduleçš„æ„é€ å‡½æ•°æ¥æ‰§è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚
        # è¿™æ ·ï¼Œåœ¨ç±»å®ä¾‹åŒ–æ—¶ä¹Ÿå¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°å‚æ•°ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°paramsï¼ˆç¨åå°†ä»‹ç»ï¼‰
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # éšè—å±‚
        self.out = nn.Linear(256, 10)  # è¾“å‡ºå±‚

    # å®šä¹‰æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥Xè¿”å›æ‰€éœ€çš„æ¨¡å‹è¾“å‡º
    def forward(self, X):
        # æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ReLUçš„å‡½æ•°ç‰ˆæœ¬ï¼Œå…¶åœ¨nn.functionalæ¨¡å—ä¸­å®šä¹‰ã€‚
        return self.out(F.relu(self.hidden(X)))
      
 
# æŸ¥çœ‹å‡½æ•°
net = MLP()
net(X)


# é¡ºåºå—
# å…³é”®å‡½æ•°
# ä¸€ç§å°†å—é€ä¸ªè¿½åŠ åˆ°åˆ—è¡¨ä¸­çš„å‡½æ•°ï¼›
# ä¸€ç§å‰å‘ä¼ æ’­å‡½æ•°ï¼Œç”¨äºå°†è¾“å…¥æŒ‰è¿½åŠ å—çš„é¡ºåºä¼ é€’ç»™å—ç»„æˆçš„â€œé“¾æ¡â€ã€‚
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # è¿™é‡Œï¼Œmoduleæ˜¯Moduleå­ç±»çš„ä¸€ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬æŠŠå®ƒä¿å­˜åœ¨'Module'ç±»çš„æˆå‘˜
            # å˜é‡_modulesä¸­ã€‚_moduleçš„ç±»å‹æ˜¯OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDictä¿è¯äº†æŒ‰ç…§æˆå‘˜æ·»åŠ çš„é¡ºåºéå†å®ƒä»¬
        for block in self._modules.values():
            X = block(X)
        return X
      
# å½“MySequentialçš„å‰å‘ä¼ æ’­å‡½æ•°è¢«è°ƒç”¨æ—¶ï¼Œ æ¯ä¸ªæ·»åŠ çš„å—éƒ½æŒ‰ç…§å®ƒä»¬è¢«æ·»åŠ çš„é¡ºåºæ‰§è¡Œã€‚ 
# ç°åœ¨å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„MySequentialç±»é‡æ–°å®ç°å¤šå±‚æ„ŸçŸ¥æœºã€‚
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)


# åœ¨å‘å‰ä¼ æ’­å‡½æ•°ä¸­æ‰§è¡Œä»£ç 
# åœ¨è¿™ä¸ªFixedHiddenMLPæ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªéšè—å±‚ï¼Œ å…¶æƒé‡ï¼ˆself.rand_weightï¼‰åœ¨å®ä¾‹åŒ–æ—¶è¢«éšæœºåˆå§‹åŒ–ï¼Œä¹‹åä¸ºå¸¸é‡ã€‚ ã€
# è¿™ä¸ªæƒé‡ä¸æ˜¯ä¸€ä¸ªæ¨¡å‹å‚æ•°ï¼Œå› æ­¤å®ƒæ°¸è¿œä¸ä¼šè¢«åå‘ä¼ æ’­æ›´æ–°ã€‚ ç„¶åï¼Œç¥ç»ç½‘ç»œå°†è¿™ä¸ªå›ºå®šå±‚çš„è¾“å‡ºé€šè¿‡ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚

# æ³¨æ„ï¼Œåœ¨è¿”å›è¾“å‡ºä¹‹å‰ï¼Œæ¨¡å‹åšäº†ä¸€äº›ä¸å¯»å¸¸çš„äº‹æƒ…ï¼š å®ƒè¿è¡Œäº†ä¸€ä¸ªwhileå¾ªç¯ï¼Œåœ¨ ğ¿1èŒƒæ•°å¤§äº 1 çš„æ¡ä»¶ä¸‹ï¼Œ 
# å°†è¾“å‡ºå‘é‡é™¤ä»¥ 2ï¼Œç›´åˆ°å®ƒæ»¡è¶³æ¡ä»¶ä¸ºæ­¢ã€‚ æœ€åï¼Œæ¨¡å‹è¿”å›äº†Xä¸­æ‰€æœ‰é¡¹çš„å’Œã€‚ 
# æ³¨æ„ï¼Œæ­¤æ“ä½œå¯èƒ½ä¸ä¼šå¸¸ç”¨äºåœ¨ä»»ä½•å®é™…ä»»åŠ¡ä¸­ï¼Œ æˆ‘ä»¬åªå±•ç¤ºå¦‚ä½•å°†ä»»æ„ä»£ç é›†æˆåˆ°ç¥ç»ç½‘ç»œè®¡ç®—çš„æµç¨‹ä¸­ã€‚
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸è®¡ç®—æ¢¯åº¦çš„éšæœºæƒé‡å‚æ•°ã€‚å› æ­¤å…¶åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸å˜
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # ä½¿ç”¨åˆ›å»ºçš„å¸¸é‡å‚æ•°ä»¥åŠreluå’Œmmå‡½æ•°
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # å¤ç”¨å…¨è¿æ¥å±‚ã€‚è¿™ç›¸å½“äºä¸¤ä¸ªå…¨è¿æ¥å±‚å…±äº«å‚æ•°
        X = self.linear(X)
        # æ§åˆ¶æµ
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
      
net = FixedHiddenMLP()
net(X)


# æ··åˆæ­é…å„ç§ç»„åˆå—çš„æ–¹æ³•
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)


# å®ç°ä¸€ä¸ªå—ï¼Œå®ƒä»¥ä¸¤ä¸ªå—ä¸ºå‚æ•°ï¼Œä¾‹å¦‚net1å’Œnet2ï¼Œå¹¶è¿”å›å‰å‘ä¼ æ’­ä¸­ä¸¤ä¸ªç½‘ç»œçš„ä¸²è”è¾“å‡ºã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºå¹³è¡Œå—ã€‚
class MySeq(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # è¿™é‡Œï¼Œmoduleæ˜¯Moduleå­ç±»çš„ä¸€ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬æŠŠå®ƒä¿å­˜åœ¨'Module'ç±»çš„æˆå‘˜
            # å˜é‡_modulesä¸­ã€‚_moduleçš„ç±»å‹æ˜¯OrderedDict
            self._modules[str(idx)] = module
            
    def forward(self, X):
        for block in self._modules.values():
            X += block(X)
        return X
n = MySeq()
n(X)
